{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current tracking uri: http://mlflow:5000\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "tracking_uri = mlflow.get_tracking_uri()\n",
    "print(\"Current tracking uri: {}\".format(tracking_uri))\n",
    "\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = \"NER\"\n",
    "# os.environ[\"MLFLOW_FLATTEN_PARAMS\"] = \"True\"\n",
    "\n",
    "# os.environ[\"MLFLOW_TRACKING_URI\"] = tracking_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90361c3d2414498497b5b322d9b59c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb41c331515453ea1f0f3dc711bdafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec1efdd5bac401fa1846a722aafe24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/198k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a590b1ced280476d81941d2da05c0a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fea36356c0444e86b2e75ae7a3e95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 394\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"kosta-naumenko/medflex\", split='train', download_mode='force_redownload', verification_mode='no_checks')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexyalunin/RuBioRoBERTa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True, \n",
    "        max_length=512, padding=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6633597563574bd5ae44a39792fef6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = raw_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " гликемия 12-13 ммоль/л, \n",
      " ИМТ 26,53 кг/м 2 \n",
      " сухость во рту, \n",
      " жажды, \n",
      " учащённого мочеиспускания. \n",
      " гликемия при контроле 1 раз в день натощак 12-13 ммоль/л,"
     ]
    }
   ],
   "source": [
    "id = 4\n",
    "input_ids, attention_mask, labels = list(tokenized_dataset[id].values())\n",
    "for i in range(len(input_ids)):\n",
    "    if labels[i] > 0:\n",
    "        if labels[i] == 1:\n",
    "            print(\" \")\n",
    "        print(tokenizer.decode(input_ids[i]), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "label_list = ['O', 'B', 'I']\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B\",\n",
    "    2: \"I\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B\": 1,\n",
    "    \"I\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "eval_dataset = tokenized_dataset.train_test_split(test_size=0.1)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at alexyalunin/RuBioRoBERTa and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map={'': torch.cuda.current_device()},\n",
    "    cache_dir='.cache',\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    )\n",
    "\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knaumenko/.conda/envs/NER/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1300' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1300/1300 22:49, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.645400</td>\n",
       "      <td>0.607616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.768536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.623800</td>\n",
       "      <td>0.575525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.776257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.555004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.779065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.537695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.784154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.534800</td>\n",
       "      <td>0.526379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.787488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.470200</td>\n",
       "      <td>0.518981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.786435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.548600</td>\n",
       "      <td>0.511208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.563400</td>\n",
       "      <td>0.508320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.790471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.532400</td>\n",
       "      <td>0.503448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.792928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.509400</td>\n",
       "      <td>0.498862</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.003344</td>\n",
       "      <td>0.794858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.524800</td>\n",
       "      <td>0.497114</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>0.795122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.527900</td>\n",
       "      <td>0.495184</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>0.005822</td>\n",
       "      <td>0.797666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.553700</td>\n",
       "      <td>0.492049</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.008798</td>\n",
       "      <td>0.799596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.487991</td>\n",
       "      <td>0.009368</td>\n",
       "      <td>0.016461</td>\n",
       "      <td>0.011940</td>\n",
       "      <td>0.799070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.534400</td>\n",
       "      <td>0.488136</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>0.016978</td>\n",
       "      <td>0.798193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.439600</td>\n",
       "      <td>0.491132</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>0.799421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.420400</td>\n",
       "      <td>0.486066</td>\n",
       "      <td>0.013405</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>0.798543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.456600</td>\n",
       "      <td>0.487221</td>\n",
       "      <td>0.015291</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>0.017544</td>\n",
       "      <td>0.798982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.473400</td>\n",
       "      <td>0.489697</td>\n",
       "      <td>0.012739</td>\n",
       "      <td>0.016461</td>\n",
       "      <td>0.014363</td>\n",
       "      <td>0.797227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.449300</td>\n",
       "      <td>0.484119</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.019048</td>\n",
       "      <td>0.800913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.458800</td>\n",
       "      <td>0.481136</td>\n",
       "      <td>0.013921</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.802755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.482500</td>\n",
       "      <td>0.480907</td>\n",
       "      <td>0.013187</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.017192</td>\n",
       "      <td>0.803633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.454300</td>\n",
       "      <td>0.482150</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>0.801965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.501700</td>\n",
       "      <td>0.482832</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>0.016584</td>\n",
       "      <td>0.801965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.562000</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.018663</td>\n",
       "      <td>0.801614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.478547</td>\n",
       "      <td>0.012959</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.016997</td>\n",
       "      <td>0.803018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.497900</td>\n",
       "      <td>0.478564</td>\n",
       "      <td>0.015801</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.803369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.396100</td>\n",
       "      <td>0.477707</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.017570</td>\n",
       "      <td>0.803896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.477651</td>\n",
       "      <td>0.018282</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.804685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>0.477831</td>\n",
       "      <td>0.013228</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>0.016103</td>\n",
       "      <td>0.803545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.478482</td>\n",
       "      <td>0.016827</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.021244</td>\n",
       "      <td>0.803808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>0.476444</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.024357</td>\n",
       "      <td>0.805300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.481700</td>\n",
       "      <td>0.476863</td>\n",
       "      <td>0.015453</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.804861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.466200</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.016563</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022039</td>\n",
       "      <td>0.806177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.476822</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.019886</td>\n",
       "      <td>0.805387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.476181</td>\n",
       "      <td>0.015982</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>0.805738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.481500</td>\n",
       "      <td>0.474926</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.025873</td>\n",
       "      <td>0.805914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.553900</td>\n",
       "      <td>0.474399</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.026490</td>\n",
       "      <td>0.805300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.474830</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.023121</td>\n",
       "      <td>0.805475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>0.474222</td>\n",
       "      <td>0.017928</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>0.806440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.504300</td>\n",
       "      <td>0.474544</td>\n",
       "      <td>0.012876</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.016925</td>\n",
       "      <td>0.804861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.473552</td>\n",
       "      <td>0.016032</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.021563</td>\n",
       "      <td>0.806440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.424600</td>\n",
       "      <td>0.472906</td>\n",
       "      <td>0.014614</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.019391</td>\n",
       "      <td>0.806353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.473536</td>\n",
       "      <td>0.016908</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.021309</td>\n",
       "      <td>0.805651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.429200</td>\n",
       "      <td>0.473297</td>\n",
       "      <td>0.022599</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.806002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.490100</td>\n",
       "      <td>0.472413</td>\n",
       "      <td>0.019149</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025245</td>\n",
       "      <td>0.806616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.517700</td>\n",
       "      <td>0.473364</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022631</td>\n",
       "      <td>0.806967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.492900</td>\n",
       "      <td>0.473202</td>\n",
       "      <td>0.016461</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.021948</td>\n",
       "      <td>0.807932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.534100</td>\n",
       "      <td>0.474012</td>\n",
       "      <td>0.016985</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022409</td>\n",
       "      <td>0.807142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.473440</td>\n",
       "      <td>0.017094</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022504</td>\n",
       "      <td>0.806440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.451200</td>\n",
       "      <td>0.472687</td>\n",
       "      <td>0.018832</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.025840</td>\n",
       "      <td>0.806002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.460300</td>\n",
       "      <td>0.472788</td>\n",
       "      <td>0.017131</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022535</td>\n",
       "      <td>0.806528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>0.471331</td>\n",
       "      <td>0.017467</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022825</td>\n",
       "      <td>0.806967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.473550</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.021572</td>\n",
       "      <td>0.807230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.588200</td>\n",
       "      <td>0.472074</td>\n",
       "      <td>0.016471</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.020958</td>\n",
       "      <td>0.806879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.470277</td>\n",
       "      <td>0.018219</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.024423</td>\n",
       "      <td>0.807581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.548900</td>\n",
       "      <td>0.471110</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.023704</td>\n",
       "      <td>0.806879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>0.470375</td>\n",
       "      <td>0.018219</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.024423</td>\n",
       "      <td>0.807669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>0.469539</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.027435</td>\n",
       "      <td>0.807844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.471105</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.806791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.465500</td>\n",
       "      <td>0.469492</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.806791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.490300</td>\n",
       "      <td>0.469513</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.027663</td>\n",
       "      <td>0.808107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.475500</td>\n",
       "      <td>0.469932</td>\n",
       "      <td>0.017817</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.023121</td>\n",
       "      <td>0.807142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>0.468937</td>\n",
       "      <td>0.020747</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.808107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.438900</td>\n",
       "      <td>0.469621</td>\n",
       "      <td>0.017582</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022923</td>\n",
       "      <td>0.807932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.415800</td>\n",
       "      <td>0.469241</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.028011</td>\n",
       "      <td>0.807844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.469047</td>\n",
       "      <td>0.021142</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.807493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.516800</td>\n",
       "      <td>0.469195</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.017778</td>\n",
       "      <td>0.807318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.358400</td>\n",
       "      <td>0.467994</td>\n",
       "      <td>0.020534</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.807844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>0.467995</td>\n",
       "      <td>0.021097</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.027894</td>\n",
       "      <td>0.807756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.467859</td>\n",
       "      <td>0.019565</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>0.807493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.529300</td>\n",
       "      <td>0.468342</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.808020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.511100</td>\n",
       "      <td>0.467176</td>\n",
       "      <td>0.020367</td>\n",
       "      <td>0.041152</td>\n",
       "      <td>0.027248</td>\n",
       "      <td>0.807493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.461600</td>\n",
       "      <td>0.468527</td>\n",
       "      <td>0.020179</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.026125</td>\n",
       "      <td>0.806879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.536200</td>\n",
       "      <td>0.468142</td>\n",
       "      <td>0.018987</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025105</td>\n",
       "      <td>0.807581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.498700</td>\n",
       "      <td>0.469280</td>\n",
       "      <td>0.018223</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.806967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.468452</td>\n",
       "      <td>0.015351</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.807318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.468302</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025175</td>\n",
       "      <td>0.808546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.456700</td>\n",
       "      <td>0.469067</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.807405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.471800</td>\n",
       "      <td>0.469016</td>\n",
       "      <td>0.015119</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>0.807669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.468666</td>\n",
       "      <td>0.015021</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.019746</td>\n",
       "      <td>0.808107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.505600</td>\n",
       "      <td>0.468856</td>\n",
       "      <td>0.017058</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.807493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.566500</td>\n",
       "      <td>0.468717</td>\n",
       "      <td>0.016913</td>\n",
       "      <td>0.032922</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.807669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.487000</td>\n",
       "      <td>0.468245</td>\n",
       "      <td>0.019149</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025245</td>\n",
       "      <td>0.808283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.450600</td>\n",
       "      <td>0.468224</td>\n",
       "      <td>0.019190</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.808020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.420400</td>\n",
       "      <td>0.468530</td>\n",
       "      <td>0.015284</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.019971</td>\n",
       "      <td>0.807493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>0.468377</td>\n",
       "      <td>0.019565</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>0.808107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.462300</td>\n",
       "      <td>0.467919</td>\n",
       "      <td>0.018367</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.024557</td>\n",
       "      <td>0.808283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.613300</td>\n",
       "      <td>0.469039</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>0.028807</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>0.807844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.477300</td>\n",
       "      <td>0.467987</td>\n",
       "      <td>0.018750</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.808195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.468439</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.807932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.468154</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.807932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.447000</td>\n",
       "      <td>0.468088</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.808107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.468224</td>\n",
       "      <td>0.019313</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025388</td>\n",
       "      <td>0.807844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.458400</td>\n",
       "      <td>0.468342</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025532</td>\n",
       "      <td>0.807932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.469100</td>\n",
       "      <td>0.468612</td>\n",
       "      <td>0.019694</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025714</td>\n",
       "      <td>0.807844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.591800</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.019912</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025899</td>\n",
       "      <td>0.807932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.467800</td>\n",
       "      <td>0.468450</td>\n",
       "      <td>0.019355</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.808020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.468414</td>\n",
       "      <td>0.019313</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025388</td>\n",
       "      <td>0.807932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.539500</td>\n",
       "      <td>0.468387</td>\n",
       "      <td>0.019313</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.025388</td>\n",
       "      <td>0.808020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cur_run_id = 1\n",
    "num_train_epochs=100\n",
    "\n",
    "name = \"RuBioRoBERTa-finetune-head\"\n",
    "run_name = f'{name}-{cur_run_id:02}'\n",
    "output_dir = f'./logs/{run_name}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"mlflow\",\n",
    "    run_name=run_name\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = f'../../models/{run_name}.pt'\n",
    "torch.save(model, path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torch.load('../../models/rubio_frozen.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.0017035775127768314,\n",
       " 'recall': 0.003173164097914778,\n",
       " 'f1': 0.002216943784639747,\n",
       " 'accuracy': 0.7840474930935167}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor(tokenized_dataset['input_ids']).to(model.device)\n",
    "attention_mask = torch.LongTensor(tokenized_dataset['attention_mask']).to(model.device)\n",
    "\n",
    "preds = model2(input_ids=input_ids, attention_mask=attention_mask)\n",
    "p = [preds['logits'].detach().cpu(), tokenized_dataset['labels']]\n",
    "compute_metrics(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.013938594838952722,\n",
       " 'recall': 0.033544877606527655,\n",
       " 'f1': 0.019693945442448436,\n",
       " 'accuracy': 0.8131428907306177}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor(tokenized_dataset['input_ids']).to(model.device)\n",
    "attention_mask = torch.LongTensor(tokenized_dataset['attention_mask']).to(model.device)\n",
    "\n",
    "preds = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "p = [preds['logits'].detach().cpu(), tokenized_dataset['labels']]\n",
    "compute_metrics(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = preds.logits.argmax(axis=2).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2697)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(logits == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
