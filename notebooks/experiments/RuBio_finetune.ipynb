{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import transformers\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54cd7751c9134b55bbda313e947156b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/883 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adaeed0b1244564925ce403ce925009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48e2cd8b6034d199061765823253c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/198k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd9ae8fd6f546a0b352904dcdb69136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861b0ae84c134eb18d2eddb310d099f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 394\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"kosta-naumenko/medflex\", split='train', download_mode='force_redownload', verification_mode='no_checks')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alexyalunin/RuBioRoBERTa\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Отмечает постепенный набор массы тела с 30 лет, в настоящее время вес максимальный -102кг ( ИМТ=32,19 кг/м 2). Неоднократно предпринимал попытки снижения веса с помощью диет и физических нагрузок с положительным временным эффектом.\\nВ 1999г. при плановом обследовании выявлено повышение гликемии до 12 ммоль/л натощак. Диагностирован СД2 типа, назначен Сиофор 1500мг вечером. В 2018г. амбулаторно проведена коррекция терапии: ЯнуМет 1000+50мг утром и вечером, Сиофор 1000мг вечером. Контроль гликемии не проводит.\\n</s>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_dataset[0][\"tokens\"], is_split_into_words=True)\n",
    "tokenizer.decode(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_dataset[0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True, \n",
    "        max_length=512, padding=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef914c69609e4bf9bdf6f47068adfebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = raw_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Семейный анамнез по эндокринным заболеваниям отягощен: у сестры – сахарный диабет 2 типа (смерть в 65 лет от рака прямой кишки).\n",
      "Сахарный диабет диагностирован около 25 лет назад ( гликемия 12-13 ммоль/л, 51 год, вес 74 кг, ИМТ 26,53 кг/м 2 ) при обследовании по поводу жалоб на сухость во рту,\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(raw_dataset[4]['tokens'][:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " гликемия 12-13 ммоль/л, \n",
      " ИМТ 26,53 кг/м 2 \n",
      " сухость во рту, \n",
      " жажды, \n",
      " учащённого мочеиспускания. \n",
      " гликемия при контроле 1 раз в день натощак 12-13 ммоль/л,"
     ]
    }
   ],
   "source": [
    "id = 4\n",
    "input_ids, attention_mask, labels = list(tokenized_dataset[id].values())\n",
    "for i in range(len(input_ids)):\n",
    "    if labels[i] > 0:\n",
    "        if labels[i] == 1:\n",
    "            print(\" \")\n",
    "        print(tokenizer.decode(input_ids[i]), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Семейный',\n",
       " 'анамнез',\n",
       " 'по',\n",
       " 'эндокринным',\n",
       " 'заболеваниям',\n",
       " 'отягощен:',\n",
       " 'у',\n",
       " 'сестры',\n",
       " '–',\n",
       " 'сахарный']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[4]['tokens'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Семейный анамнез по эндокринным заболеваниям отягощен: у сестры – сахарный диабет 2 типа (смерть в 65 лет от рака прямой кишки).\n",
      "Сахарный диабет диагностирован около 25 лет назад ( гликемия 12-13 ммоль/л, 51 год, вес 74 кг, ИМТ 26,53 кг/м 2 ) при обследовании по поводу жалоб на сухость во рту,\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(raw_dataset[4]['tokens'][:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Семейный анамнез по эндокринным заболеваниям отягощен: у сестры – сахарный диабет 2 типа (смерть в 65 лет от рака прямой кишки).\n",
      "Сахарный диабет диагностирован около 25 лет назад (гликемия 12-13 ммоль/л, 51 год, вес 74 кг, ИМТ 26,53 кг/м 2 ) при обследовании по поводу жалоб на сухость во рту, наличие жажды, учащённого мочеиспускания. Назначена таблетированная сахароснижающая терапия (названия препаратов и дозы назвать затрудняется). На фоне терапии отмечалось улучшение показателей гликемии, но стойкой компенсации достигнуто не было. В дальнейшем неоднократно проводилась коррекция лечения (название препаратов и дозы назвать затрудняется). В настоящее время получает терапию по схеме: метформин 850 мг 2 раза в день, Диабетон 60 мг, Туджео 16 ЕД вечером. На этом фоне гликемия при контроле 1 раз в день натощак 12-13 ммоль/л, в дневное время не контролирует\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Семейный анамнез по эндокринным заболеваниям отягощен: у сестры – сахарный диабет 2 типа (смерть в 65 лет от рака прямой кишки).\n",
    "Сахарный диабет диагностирован около 25 лет назад (гликемия 12-13 ммоль/л, 51 год, вес 74 кг, ИМТ 26,53 кг/м 2 ) при обследовании по поводу жалоб на сухость во рту, наличие жажды, учащённого мочеиспускания. Назначена таблетированная сахароснижающая терапия (названия препаратов и дозы назвать затрудняется). На фоне терапии отмечалось улучшение показателей гликемии, но стойкой компенсации достигнуто не было. В дальнейшем неоднократно проводилась коррекция лечения (название препаратов и дозы назвать затрудняется). В настоящее время получает терапию по схеме: метформин 850 мг 2 раза в день, Диабетон 60 мг, Туджео 16 ЕД вечером. На этом фоне гликемия при контроле 1 раз в день натощак 12-13 ммоль/л, в дневное время не контролирует\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Семейный анамнез по эндокринным заболеваниям отягощен: у сестры – сахарный диабет 2 типа (смерть в 65 лет от рака прямой кишки).\n",
      "Сахарный диабет диагностирован около 25 лет назад ( гликемия 12-13 ммоль/л, 51 год, вес 74 кг, ИМТ 26,53 кг/м 2 ) при обследовании по поводу жалоб на сухость во рту, наличие жажды, учащённого мочеиспускания. Назначена таблетированная сахароснижающая терапия (названия препаратов и дозы назвать затрудняется). На фоне терапии отмечалось улучшение показателей гликемии,\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_dataset[4]['input_ids'])[4:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\n",
    "# [tokens],\n",
    "# [tokens],\n",
    "# [tokens],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(raw_dataset[4]['ner_tags'][:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[4]['labels'][:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")\n",
    "label_list = ['O', 'B', 'I']\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B\",\n",
    "    2: \"I\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B\": 1,\n",
    "    \"I\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at alexyalunin/RuBioRoBERTa and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    device_map={'': torch.cuda.current_device()},\n",
    "    cache_dir='.cache',\n",
    "    num_labels=3,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    "    )\n",
    "\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "max_len = 512\n",
    "num = 0\n",
    "for row in tokenized_dataset['labels']:\n",
    "    if len(row) > max_len:\n",
    "        num += 1\n",
    "        # max_len = len(row)\n",
    "print(max_len)\n",
    "print(num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (1) is identical to the `bos_token_id` (1), `eos_token_id` (2), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.0083, -0.3615, -0.4206],\n",
       "          [-0.2646, -0.0017,  0.7536],\n",
       "          [ 0.0159, -0.0583, -0.2419],\n",
       "          ...,\n",
       "          [ 0.6841, -1.0235, -0.5859],\n",
       "          [ 0.4109, -0.6313, -0.6629],\n",
       "          [ 0.9985, -0.1974, -1.0233]]], device='cuda:0',\n",
       "        grad_fn=<ViewBackward0>),)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenized_dataset['input_ids'][0]\n",
    "inputs = torch.Tensor([tokens]).long()\n",
    "inputs = inputs.to(model.device)\n",
    "model(inputs)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knaumenko/.conda/envs/NER/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.518800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.507000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.528300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.5188696479797363, metrics={'train_runtime': 22.1816, 'train_samples_per_second': 35.525, 'train_steps_per_second': 4.508, 'total_flos': 731823591051264.0, 'train_loss': 0.5188696479797363, 'epoch': 2.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.05,\n",
    "    logging_steps=20,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # save_strategy=\"epoch\",\n",
    "    # load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.0037313432835820895,\n",
       " 'recall': 0.011786038077969175,\n",
       " 'f1': 0.005668192718552431,\n",
       " 'accuracy': 0.7871627578910245}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model(torch.LongTensor(tokenized_dataset['input_ids']).to(model.device))\n",
    "p = [preds['logits'].detach().cpu(), tokenized_dataset['labels']]\n",
    "compute_metrics(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'precision': 0.00025539522410930913,\n",
    " 'recall': 0.0009062075215224287,\n",
    " 'f1': 0.00039848575413428965,\n",
    " 'accuracy': 0.7268569749791901}\n",
    "\n",
    "{'precision': 0.006167613979925021,\n",
    " 'recall': 0.023108291798821932,\n",
    " 'f1': 0.009736540664375716,\n",
    " 'accuracy': 0.7841453263477451}\n",
    "\n",
    "{'precision': 0.007338551859099804,\n",
    " 'recall': 0.02718622564567286,\n",
    " 'f1': 0.011557353366079168,\n",
    " 'accuracy': 0.7893355530529306}\n",
    "\n",
    "{'precision': 0.008235145065247688,\n",
    " 'recall': 0.02945174444947893,\n",
    " 'f1': 0.012871287128712872,\n",
    " 'accuracy': 0.7918033589580376}\n",
    "\n",
    "{'precision': 0.008580441640378548,\n",
    " 'recall': 0.030811055731762575,\n",
    " 'f1': 0.013422818791946308,\n",
    " 'accuracy': 0.7921167311364637}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'rubio_frozen.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('rubio_frozen.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
